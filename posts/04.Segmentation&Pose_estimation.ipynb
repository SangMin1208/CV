{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9a491ad5-a7a5-4763-b55d-760ba9715417",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"4. Segmentation, Pose estimation\"\n",
    "author: \"이상민\"\n",
    "date: \"01/24/2026\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db577820-e495-45ab-a0e8-faeb0f3517e9",
   "metadata": {},
   "source": [
    "## 1. Segmentation\n",
    "\n",
    "### 1) Transposed Convolution\n",
    "\n",
    "* 주로 upsampling $\\rightarrow$ input 보다 output의 공간이 더 큼\n",
    "\n",
    "\n",
    "<img src = \"../assets/3-3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9f7706-4c95-435f-9259-f542fc970d0f",
   "metadata": {},
   "source": [
    "### 2) Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4ce7be-2af7-49d5-a653-5f9c9313d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5012dda3-3f48-48b8-b5f1-5ae4e68a6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899cc596-f545-4b14-a5c0-8d7af50ebc23",
   "metadata": {},
   "source": [
    "* SegNet에서는 각 픽셀이 class 에 속할 확률 예측을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15232b42-76ae-4011-b24b-0ff99c978004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegNet(nn.Module):\n",
    "    def __init__(self, numObj):\n",
    "        super(SegNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 7)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, numObj, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e809f547-f427-4ec2-b2eb-95ea5d9509b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000):\n",
    "        self.num_samples = num_samples\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.randn(1, 28, 28)\n",
    "        mask = torch.randint(0, numObj, (28, 28))\n",
    "        return {'image': image, 'mask': mask}\n",
    "\n",
    "train_dataset = DummyDataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7310abd8-819e-49ec-8d1f-3978a779b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "\n",
    "\n",
    "numObj = 10\n",
    "model = SegNet(numObj)\n",
    "model.train()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch in train_dataloader:\n",
    "        input = torch.autograd.Variable(batch['image'])\n",
    "        target = torch.autograd.Variable(batch['mask'])\n",
    "        \n",
    "        predicted = model(input)\n",
    "        output = torch.nn.functional.softmax(predicted, dim=1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d59b127-b494-4d68-b7ab-98786c03e023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegNet(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(64, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose2d(16, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96873622-a9ca-4edd-8ca8-8531b171e0c8",
   "metadata": {},
   "source": [
    "## 2. Image generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68b98fe-0615-4d74-8625-553a2bf1f0a8",
   "metadata": {},
   "source": [
    "* Directly predict the missing part : with the advancement of adversarial loss\n",
    "\n",
    "<img src = '../assets/3-4.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3bfb87f-1060-49d4-8a4b-9bef2e01d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPM2DPose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CPM2DPose, self).__init__()\n",
    "        self.relu = F.leaky_relu\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv4_3 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv4_4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv4_5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv4_6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv4_7 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv5_1 = nn.Conv2d(128, 512, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.conv5_2 = nn.Conv2d(512, 21, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.conv6_1 = nn.Conv2d(149, 128, kernel_size=7, stride=1, padding=3, bias=True)\n",
    "        self.conv6_2 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n",
    "        self.conv6_3 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n",
    "        self.conv6_4 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n",
    "        self.conv6_5 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n",
    "        self.conv6_6 = nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.conv6_7 = nn.Conv2d(128, 21, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.conv7_1 = nn.Conv2d(149, 128, kernel_size=7, stride=1, padding=3, bias=True)\n",
    "        self.conv7_2 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n",
    "        self.conv7_3 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n",
    "        self.conv7_4 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n",
    "        self.conv7_5 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n",
    "        self.conv7_6 = nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.conv7_7 = nn.Conv2d(128, 21, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1_1(x))\n",
    "        x = self.relu(self.conv1_2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv2_1(x))\n",
    "        x = self.relu(self.conv2_2(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv3_1(x))\n",
    "        x = self.relu(self.conv3_2(x))\n",
    "        x = self.relu(self.conv3_3(x))\n",
    "        x = self.relu(self.conv3_4(x))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.relu(self.conv4_1(x))\n",
    "        x = self.relu(self.conv4_2(x))\n",
    "        x = self.relu(self.conv4_3(x))\n",
    "        x = self.relu(self.conv4_4(x))\n",
    "        x = self.relu(self.conv4_5(x))\n",
    "        x = self.relu(self.conv4_6(x))\n",
    "        encoding = self.relu(self.conv4_7(x))\n",
    "        x = self.relu(self.conv5_1(encoding))\n",
    "        scoremap = self.conv5_2(x)\n",
    "        x = torch.cat([scoremap, encoding],1)\n",
    "        x = self.relu(self.conv6_1(x))\n",
    "        x = self.relu(self.conv6_2(x))\n",
    "        x = self.relu(self.conv6_3(x))\n",
    "        x = self.relu(self.conv6_4(x))\n",
    "        x = self.relu(self.conv6_5(x))\n",
    "        x = self.relu(self.conv6_6(x))\n",
    "        scoremap = self.conv6_7(x)\n",
    "        x = torch.cat([scoremap, encoding], 1)\n",
    "        x = self.relu(self.conv7_1(x))\n",
    "        x = self.relu(self.conv7_2(x))\n",
    "        x = self.relu(self.conv7_3(x))\n",
    "        x = self.relu(self.conv7_4(x))\n",
    "        x = self.relu(self.conv7_5(x))\n",
    "        x = self.relu(self.conv7_6(x))\n",
    "        x = self.conv7_7(x)\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
