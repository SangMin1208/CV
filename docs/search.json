[
  {
    "objectID": "posts/02.Review_on_deep_learning.html",
    "href": "posts/02.Review_on_deep_learning.html",
    "title": "2. Review on Deep Learning",
    "section": "",
    "text": "- Shallow learning\n\nonly has one layer\nfeature : vector\n\n- Deep learning\n\nhas more than one layer\nuse Gradient Descent\nover fitting problem\n\n\nimput image(224 x 224 x 3) : width x height x channel\n\n\n\n- Extract optimal representation via End-to-End learning\n\n머신러닝은 각각의 task마다 새로운 모델 design 필요\n딥러닝은 다른 task에서도 괜찮은 표현력\n\n- Non-linearity\n\nDeep learning can represent non-linear data\nMost computer vision applications invlove data which requires non-linear mappings\n\n- 내가 생각한 고전 ML과 비교한 장점\n\nEnd-to-End learning\n\n자동으로 feature를 추출해서 학습\n새로운 task에 쉽게 적응가능(transfer learning)\n\n고전 ML은 일정수준에서 성능 향상 x, 딥러닝은 데이터와 코델 크기 커질수록 계속 성능 향상"
  },
  {
    "objectID": "posts/02.Review_on_deep_learning.html#deep-vs.-machine-learning",
    "href": "posts/02.Review_on_deep_learning.html#deep-vs.-machine-learning",
    "title": "2. Review on Deep Learning",
    "section": "",
    "text": "- Shallow learning\n\nonly has one layer\nfeature : vector\n\n- Deep learning\n\nhas more than one layer\nuse Gradient Descent\nover fitting problem\n\n\nimput image(224 x 224 x 3) : width x height x channel\n\n\n\n- Extract optimal representation via End-to-End learning\n\n머신러닝은 각각의 task마다 새로운 모델 design 필요\n딥러닝은 다른 task에서도 괜찮은 표현력\n\n- Non-linearity\n\nDeep learning can represent non-linear data\nMost computer vision applications invlove data which requires non-linear mappings\n\n- 내가 생각한 고전 ML과 비교한 장점\n\nEnd-to-End learning\n\n자동으로 feature를 추출해서 학습\n새로운 task에 쉽게 적응가능(transfer learning)\n\n고전 ML은 일정수준에서 성능 향상 x, 딥러닝은 데이터와 코델 크기 커질수록 계속 성능 향상"
  },
  {
    "objectID": "posts/02.Review_on_deep_learning.html#cnnsconvolutional-neural-networks",
    "href": "posts/02.Review_on_deep_learning.html#cnnsconvolutional-neural-networks",
    "title": "2. Review on Deep Learning",
    "section": "2. CNNs(Convolutional Neural Networks)",
    "text": "2. CNNs(Convolutional Neural Networks)\n# RGB image \\(\\rightarrow\\) Any Differentiable Layers(learnability) \\(\\rightarrow\\) Semantic labels\n\nA. Differentiable layers and Gradient descent\n# \\(I\\) \\(\\rightarrow\\) layer \\(h\\) \\(\\rightarrow\\) \\(X\\)\n\\(\\rightarrow\\) Any Differentiable intermediate Layer \\(f\\) whose parameter is \\(\\theta\\)\n\\(\\rightarrow\\) \\(y\\) \\(\\rightarrow\\) layer \\(g\\)(loss function) \\(\\rightarrow\\) Loss \\(L\\)(distance btw ground truth & prediction)\n\nforward rule(순전파) : \\(y = f(h(I);\\theta)\\:\\:\\) for \\(\\:\\: g(f(h(I);\\theta))=L\\)\nbackward rule(역전파) : \\(\\frac{dL}{dI} = \\frac{dX}{dI} \\times \\frac{dy}{dX} \\times \\frac{dL}{dy}\\)\nparameter update rule : \\(\\theta^{new} =  \\theta - \\epsilon(\\text{learning rate})\\frac{dy}{d\\theta}\\times\\frac{dL}{dy}\\)\n\n\n\nB. CNN architecture\n\n- 2D Convolution\n\n\n\n하이퍼 파라미터 Stride =2로 지정하면 2칸씩 움직임\n\n\n\n입력이 3채널(RGB) 이면 filter도 3채널\n각 채널마다 다른 가중치\nex) 3x3 vlfxjdlaus (3,3,3) - 27개의 가중치 존재\nn개의 filter를 사용해 n개의 feature map 생성\n입력 h,w,3 \\(\\rightarrow\\) ?,?,n\n\n- Pooling\n\n\nMax pooling & Average pooling\n\n\n\nActivation layer\n\n\n\n최종 CNN shape 요약"
  },
  {
    "objectID": "posts/04.Segmentation&Pose_estimation.html",
    "href": "posts/04.Segmentation&Pose_estimation.html",
    "title": "4. Segmentation, Pose estimation",
    "section": "",
    "text": "주로 upsampling \\(\\rightarrow\\) input 보다 output의 공간이 더 큼\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n\nclass Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 7)\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, 7),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU()\n        )\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n\nSegNet에서는 각 픽셀이 class 에 속할 확률 예측을 출력\n\n\nclass SegNet(nn.Module):\n    def __init__(self, numObj):\n        super(SegNet, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 7)\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, 7),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(16, numObj, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU()\n        )\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n\nfrom torch.utils.data import DataLoader, Dataset\n\nclass DummyDataset(Dataset):\n    def __init__(self, num_samples=1000):\n        self.num_samples = num_samples\n    def __len__(self):\n        return self.num_samples\n    def __getitem__(self, idx):\n        image = torch.randn(1, 28, 28)\n        mask = torch.randint(0, numObj, (28, 28))\n        return {'image': image, 'mask': mask}\n\ntrain_dataset = DummyDataset()\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n\nNUM_EPOCHS = 10\n\n\nnumObj = 10\nmodel = SegNet(numObj)\nmodel.train()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nfor epoch in range(NUM_EPOCHS):\n    for batch in train_dataloader:\n        input = torch.autograd.Variable(batch['image'])\n        target = torch.autograd.Variable(batch['mask'])\n        \n        predicted = model(input)\n        output = torch.nn.functional.softmax(predicted, dim=1)\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n\nmodel\n\nSegNet(\n  (encoder): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n  )\n  (decoder): Sequential(\n    (0): ConvTranspose2d(64, 32, kernel_size=(7, 7), stride=(1, 1))\n    (1): ReLU()\n    (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (3): ReLU()\n    (4): ConvTranspose2d(16, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (5): ReLU()\n  )\n)"
  },
  {
    "objectID": "posts/04.Segmentation&Pose_estimation.html#segmentation",
    "href": "posts/04.Segmentation&Pose_estimation.html#segmentation",
    "title": "4. Segmentation, Pose estimation",
    "section": "",
    "text": "주로 upsampling \\(\\rightarrow\\) input 보다 output의 공간이 더 큼\n\n\n\n\n\n\nimport torch\nimport torch.nn as nn\n\n\nclass Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 7)\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, 7),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU()\n        )\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n\nSegNet에서는 각 픽셀이 class 에 속할 확률 예측을 출력\n\n\nclass SegNet(nn.Module):\n    def __init__(self, numObj):\n        super(SegNet, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 7)\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, 7),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(16, numObj, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU()\n        )\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n\nfrom torch.utils.data import DataLoader, Dataset\n\nclass DummyDataset(Dataset):\n    def __init__(self, num_samples=1000):\n        self.num_samples = num_samples\n    def __len__(self):\n        return self.num_samples\n    def __getitem__(self, idx):\n        image = torch.randn(1, 28, 28)\n        mask = torch.randint(0, numObj, (28, 28))\n        return {'image': image, 'mask': mask}\n\ntrain_dataset = DummyDataset()\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n\nNUM_EPOCHS = 10\n\n\nnumObj = 10\nmodel = SegNet(numObj)\nmodel.train()\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nfor epoch in range(NUM_EPOCHS):\n    for batch in train_dataloader:\n        input = torch.autograd.Variable(batch['image'])\n        target = torch.autograd.Variable(batch['mask'])\n        \n        predicted = model(input)\n        output = torch.nn.functional.softmax(predicted, dim=1)\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n\nmodel\n\nSegNet(\n  (encoder): Sequential(\n    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(32, 64, kernel_size=(7, 7), stride=(1, 1))\n  )\n  (decoder): Sequential(\n    (0): ConvTranspose2d(64, 32, kernel_size=(7, 7), stride=(1, 1))\n    (1): ReLU()\n    (2): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (3): ReLU()\n    (4): ConvTranspose2d(16, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n    (5): ReLU()\n  )\n)"
  },
  {
    "objectID": "posts/04.Segmentation&Pose_estimation.html#image-generation",
    "href": "posts/04.Segmentation&Pose_estimation.html#image-generation",
    "title": "4. Segmentation, Pose estimation",
    "section": "2. Image generation",
    "text": "2. Image generation\n\nDirectly predict the missing part : with the advancement of adversarial loss\n\n\n\nclass CPM2DPose(nn.Module):\n    def __init__(self):\n        super(CPM2DPose, self).__init__()\n        self.relu = F.leaky_relu\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv4_3 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv4_4 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv4_5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv4_6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv4_7 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1, bias=True)\n        self.conv5_1 = nn.Conv2d(128, 512, kernel_size=1, stride=1, padding=0, bias=True)\n        self.conv5_2 = nn.Conv2d(512, 21, kernel_size=1, stride=1, padding=0, bias=True)\n        self.conv6_1 = nn.Conv2d(149, 128, kernel_size=7, stride=1, padding=3, bias=True)\n        self.conv6_2 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n        self.conv6_3 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n        self.conv6_4 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n        self.conv6_5 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n        self.conv6_6 = nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0, bias=True)\n        self.conv6_7 = nn.Conv2d(128, 21, kernel_size=1, stride=1, padding=0, bias=True)\n        self.conv7_1 = nn.Conv2d(149, 128, kernel_size=7, stride=1, padding=3, bias=True)\n        self.conv7_2 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n        self.conv7_3 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n        self.conv7_4 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n        self.conv7_5 = nn.Conv2d(128, 128, kernel_size=7, stride=1, padding=3, bias=True)\n        self.conv7_6 = nn.Conv2d(128, 128, kernel_size=1, stride=1, padding=0, bias=True)\n        self.conv7_7 = nn.Conv2d(128, 21, kernel_size=1, stride=1, padding=0, bias=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n\n    def forward(self, x):\n        x = self.relu(self.conv1_1(x))\n        x = self.relu(self.conv1_2(x))\n        x = self.maxpool(x)\n        x = self.relu(self.conv2_1(x))\n        x = self.relu(self.conv2_2(x))\n        x = self.maxpool(x)\n        x = self.relu(self.conv3_1(x))\n        x = self.relu(self.conv3_2(x))\n        x = self.relu(self.conv3_3(x))\n        x = self.relu(self.conv3_4(x))\n        x = self.maxpool(x)\n        x = self.relu(self.conv4_1(x))\n        x = self.relu(self.conv4_2(x))\n        x = self.relu(self.conv4_3(x))\n        x = self.relu(self.conv4_4(x))\n        x = self.relu(self.conv4_5(x))\n        x = self.relu(self.conv4_6(x))\n        encoding = self.relu(self.conv4_7(x))\n        x = self.relu(self.conv5_1(encoding))\n        scoremap = self.conv5_2(x)\n        x = torch.cat([scoremap, encoding],1)\n        x = self.relu(self.conv6_1(x))\n        x = self.relu(self.conv6_2(x))\n        x = self.relu(self.conv6_3(x))\n        x = self.relu(self.conv6_4(x))\n        x = self.relu(self.conv6_5(x))\n        x = self.relu(self.conv6_6(x))\n        scoremap = self.conv6_7(x)\n        x = torch.cat([scoremap, encoding], 1)\n        x = self.relu(self.conv7_1(x))\n        x = self.relu(self.conv7_2(x))\n        x = self.relu(self.conv7_3(x))\n        x = self.relu(self.conv7_4(x))\n        x = self.relu(self.conv7_5(x))\n        x = self.relu(self.conv7_6(x))\n        x = self.conv7_7(x)\n        return x"
  },
  {
    "objectID": "posts/01.Overview_and_OpenCV.html",
    "href": "posts/01.Overview_and_OpenCV.html",
    "title": "1. Overview and Open CV",
    "section": "",
    "text": "Understand what is computer vision/deep learning algorithms\nUnderstand how to implement image/video processing algorithms in Python using OpenCV.\nUnderstand how to implement/train/test convolutional neural networks (CNNs) and Transformer in PyTorch.\n\n\n\n\n- What is computer vision?\n\n- What is deep learning?\n\nAlphaGo is using a deep learning when training their parameters\nImageNet challenge (2012년 AlexNet(8 layers) \\(\\rightarrow\\) 2015년 ResNet(152 layers)) getting deeper\n\n\n\n\n\n- Image classification for Cifar-10 dataset.\n\nAssuming that images are composed of one main object.\n\n\n- Object detection\n\nImage could be composed of multiple objects\nDetect object location + class imformation\n\n\n- Semantic segmentation\n\nDetect object’s pixel-level location + class information\n\n\n- Image generation\n\ndraw new images by changing atrributes of the original images\n\n\n- Pose estimation\n\nRecognize 2D/3D poses of the main objects(body, hand, face)\n\n\n- 3D hand mesh reconstruction\n\n21 Keypoints(4points for finger(20 joint) + wrist(손목)(1 joint))\n778 vertices\n\n\n- 3D human mesh reconstruction\n\nabout 7000 vertices"
  },
  {
    "objectID": "posts/01.Overview_and_OpenCV.html#course-overview",
    "href": "posts/01.Overview_and_OpenCV.html#course-overview",
    "title": "1. Overview and Open CV",
    "section": "",
    "text": "Understand what is computer vision/deep learning algorithms\nUnderstand how to implement image/video processing algorithms in Python using OpenCV.\nUnderstand how to implement/train/test convolutional neural networks (CNNs) and Transformer in PyTorch.\n\n\n\n\n- What is computer vision?\n\n- What is deep learning?\n\nAlphaGo is using a deep learning when training their parameters\nImageNet challenge (2012년 AlexNet(8 layers) \\(\\rightarrow\\) 2015년 ResNet(152 layers)) getting deeper\n\n\n\n\n\n- Image classification for Cifar-10 dataset.\n\nAssuming that images are composed of one main object.\n\n\n- Object detection\n\nImage could be composed of multiple objects\nDetect object location + class imformation\n\n\n- Semantic segmentation\n\nDetect object’s pixel-level location + class information\n\n\n- Image generation\n\ndraw new images by changing atrributes of the original images\n\n\n- Pose estimation\n\nRecognize 2D/3D poses of the main objects(body, hand, face)\n\n\n- 3D hand mesh reconstruction\n\n21 Keypoints(4points for finger(20 joint) + wrist(손목)(1 joint))\n778 vertices\n\n\n- 3D human mesh reconstruction\n\nabout 7000 vertices"
  },
  {
    "objectID": "posts/01.Overview_and_OpenCV.html#open-cv",
    "href": "posts/01.Overview_and_OpenCV.html#open-cv",
    "title": "1. Overview and Open CV",
    "section": "2. Open CV",
    "text": "2. Open CV\n\nA. Image read\n- Import\n\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\n- 컬러 이미지로 불러오기 IMREAD_UNCHANGED\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n\nimg.shape\n\n(454, 680, 3)\n\n\n- 흑백 이미지로 불러오기 IMREAD_GRAYSCALE\n\nimg2 = cv2.imread('./example.jpg', cv2.IMREAD_GRAYSCALE)\n\nplt.imshow(img2, cmap = 'gray')\n\n\n\n\n\n\n\n\n\nimg2.shape\n\n(454, 680)\n\n\n\n\nB.Image processing\n- 사각형 추가하기\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\ncv2.rectangle(img, (50, 50), (150, 150), (255, 0, 0))\ncv2.rectangle(img, (300, 300), (100, 100), (0, 255, 0), 10)\ncv2.rectangle(img, (450, 200), (200, 450), (0, 0, 255), -1)\n\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- polylines 그리기\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\npts1 = np.array([[50,50], [150,150], [100,140], [200,240]], dtype=np.int32)\ncv2.polylines(img, [pts1], False, (255,0,0))\n\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- 원 그리기\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\ncv2.circle(img, (150,150), 100, (255,0,0))\ncv2.ellipse(img, ((325,300), (150,100), 0), (0,255,0), 5)\n\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- Put texts\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\ncv2.putText(img, 'UNIST CSE48001 Computer Vision!', (20,50), cv2.FONT_HERSHEY_PLAIN, 2, (0,255,0))\n\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- Image resize\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\nheight, width = img.shape[:2]\n\ndst1 = cv2.resize(img, None, None, 0.5, 0.5, cv2.INTER_CUBIC)\ndst2 = cv2.resize(img, None, None, 2, 2, cv2.INTER_CUBIC)\n\nimg_rgb_dst1 = cv2.cvtColor(dst1, cv2.COLOR_BGR2RGB)\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nimg_rgb_dst2 = cv2.cvtColor(dst2, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(5, 5))\nplt.imshow(img_rgb_dst1)\nplt.title('0.5x scaled')\nplt.axis('off')\nplt.show()\n\nplt.figure(figsize=(10, 10))\nplt.imshow(img_rgb)\nplt.title('Original')\nplt.axis('off')\nplt.show()\n\nplt.figure(figsize=(20, 20))\nplt.imshow(img_rgb_dst2)\nplt.title('2x scaled')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n- Image blurring\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\nblur1 = cv2.blur(img, (10,10))\n\nimg_rgb = cv2.cvtColor(blur1, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- Edge detection(밝기가 급격히 변하는 부분 감지)\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\nedges = cv2.Canny(img, 100, 200)\n\nimg_rgb = cv2.cvtColor(edges, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- Corner detection\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\ncorner = cv2.cornerHarris(gray, 2, 3, 0.04)\ncoord = np.where(corner &gt; 0.1*corner.max())\ncoord = np.stack((coord[1], coord[0]), axis=-1)\n\nfor x,y in coord:\n  cv2.circle(img, (x,y), 5, (0,0,255), 1, cv2.LINE_AA)\n\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- Image matching\n\nimg1 = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\nimg2 = cv2.imread('./example4.jpg', cv2.IMREAD_UNCHANGED)\n\ngray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\ngray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n\ndetector = cv2.ORB_create()\nkp1, desc1 = detector.detectAndCompute(gray1, None)\nkp2, desc2 = detector.detectAndCompute(gray2, None)\n\nmatcher = cv2.BFMatcher(cv2.NORM_L1, crossCheck = True)\nmatches = matcher.match(desc1, desc2)\n\nres = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n\nimg_rgb = cv2.cvtColor(res, cv2.COLOR_BGR2RGB)\nplt.imshow(img_rgb)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CV",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 25, 2026\n\n\n5. Object_detection_pipline\n\n\n이상민 \n\n\n\n\nJan 24, 2026\n\n\n4. Segmentation, Pose estimation\n\n\n이상민 \n\n\n\n\nJan 23, 2026\n\n\n3. Review on PyTorch\n\n\n이상민 \n\n\n\n\nJan 21, 2026\n\n\n2. Review on Deep Learning\n\n\n이상민 \n\n\n\n\nJan 20, 2026\n\n\n1. Overview and Open CV\n\n\n이상민 \n\n\n\n\nJan 1, 2026\n\n\n0. Assignment1\n\n\n이상민 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/05.Object_detection_pipeline.html",
    "href": "posts/05.Object_detection_pipeline.html",
    "title": "5. Object_detection_pipline",
    "section": "",
    "text": "Input : Image\nSelective search 알고리즘을 이용해 RoI(Regions of Interest)를 추출(약 2000개)\nImageNet으로 사전학습된 CNN으로 각각의 영역에 대해 feature vector 추출(동일 크기의 output을 위해 warp 작업으로 크기를 찌그러뜨려 동일 input size)\nSVM을 통한 Classification"
  },
  {
    "objectID": "posts/05.Object_detection_pipeline.html#r-cnncvpr13",
    "href": "posts/05.Object_detection_pipeline.html#r-cnncvpr13",
    "title": "5. Object_detection_pipline",
    "section": "",
    "text": "Input : Image\nSelective search 알고리즘을 이용해 RoI(Regions of Interest)를 추출(약 2000개)\nImageNet으로 사전학습된 CNN으로 각각의 영역에 대해 feature vector 추출(동일 크기의 output을 위해 warp 작업으로 크기를 찌그러뜨려 동일 input size)\nSVM을 통한 Classification"
  },
  {
    "objectID": "posts/05.Object_detection_pipeline.html#fast-r-cnniccv15",
    "href": "posts/05.Object_detection_pipeline.html#fast-r-cnniccv15",
    "title": "5. Object_detection_pipline",
    "section": "2. Fast R-CNN(ICCV’15)",
    "text": "2. Fast R-CNN(ICCV’15)\n\nR-CNN은 2000개의 region이 CNN을 거치므로 연산량 증가, 병목현상\n이를 개선하여 CNN을 한번만 거침 \\(\\rightarrow\\) 1개의 feature map\n원본 이미지에 selective search알고리즘 수행 후 구한 영역이 Feature Map에 Projection 되는 방식"
  },
  {
    "objectID": "posts/05.Object_detection_pipeline.html#faster-r-cnnneurips15",
    "href": "posts/05.Object_detection_pipeline.html#faster-r-cnnneurips15",
    "title": "5. Object_detection_pipline",
    "section": "3. Faster R-CNN(NeurIPS’15)",
    "text": "3. Faster R-CNN(NeurIPS’15)\n\nFast R-CNN도 selective esarch알고리즘이 CPU를 사용하기 때문에 CPU 연산량 증가\nRPN : Region proposal을 수행해주는 CNN 구조의 네트워크. \\(\\rightarrow\\) GPU에서 돌릴 수 있게 됨"
  },
  {
    "objectID": "posts/05.Object_detection_pipeline.html#lstm",
    "href": "posts/05.Object_detection_pipeline.html#lstm",
    "title": "5. Object_detection_pipline",
    "section": "4. LSTM",
    "text": "4. LSTM\n\nLong Term Short Memory\nForget gate : How much retain past info\nInput gate : How much use the new info\nOutput gate: How much use the new output\nBecome robust by learning how to forget and retain"
  },
  {
    "objectID": "posts/05.Object_detection_pipeline.html#seq2seq",
    "href": "posts/05.Object_detection_pipeline.html#seq2seq",
    "title": "5. Object_detection_pipline",
    "section": "5. Seq2seq",
    "text": "5. Seq2seq"
  },
  {
    "objectID": "posts/00.Assignment1.html",
    "href": "posts/00.Assignment1.html",
    "title": "0. Assignment1",
    "section": "",
    "text": "1. Using the OpenCV library, please write a code (Prob1.py) for drawing clock images given two arguments (ie. hour, minute) and meeting the conditions, as follows\n\n\"\"\"\nProb1.py - 시계 이미지 생성기\nOpenCV를 사용하여 주어진 시(hour)와 분(minute)에 해당하는 시계 이미지를 생성합니다.\n랜덤 색상, 노이즈, 크기 조정, 이동 등의 조건을 충족합니다.\n\"\"\"\n\nimport cv2\nimport numpy as np\nimport math\nimport os\n\n# ===== 상수 정의 =====\nIMG_SIZE = 227          # 이미지 크기 (227x227x3)\nFONT = cv2.FONT_HERSHEY_SIMPLEX\nFONT_SCALE = 0.5\nFONT_THICKNESS = 1\nHOUR_HAND_THICKNESS = 5    # 시침 두께\nMINUTE_HAND_THICKNESS = 3  # 분침 두께\n\n\ndef generate_random_color(exclude_color=None):\n    \"\"\"\n    랜덤 색상 생성 (검정색 제외).\n    exclude_color가 주어지면 해당 색상과 충분히 다른 색상을 생성합니다.\n    \"\"\"\n    while True:\n        color = tuple(np.random.randint(30, 256, 3).tolist())\n        if exclude_color is None:\n            return color\n        # 두 색상 간의 유클리드 거리가 충분히 큰지 확인\n        diff = sum((a - b) ** 2 for a, b in zip(color, exclude_color)) ** 0.5\n        if diff &gt; 80:\n            return color\n\n\ndef add_noise(img):\n    \"\"\"\n    이미지에 랜덤 노이즈를 추가합니다.\n    노이즈 레벨은 확률에 따라 선택됩니다.\n    \"\"\"\n    noise_level = np.random.choice([1, 64, 128, 192, 256], p=[0.1, 0.2, 0.4, 0.2, 0.1])\n    noise = np.random.randint(0, noise_level, (IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n    img = cv2.add(img, noise)\n    return img\n\n\ndef get_hand_endpoint(cx, cy, length, angle_deg):\n    \"\"\"\n    시계 중심(cx, cy)에서 주어진 길이와 각도로 바늘 끝점 좌표를 계산합니다.\n    angle_deg: 12시 방향을 0도로 하여 시계 방향으로 증가\n    \"\"\"\n    angle_rad = math.radians(angle_deg - 90)\n    ex = int(cx + length * math.cos(angle_rad))\n    ey = int(cy + length * math.sin(angle_rad))\n    return (ex, ey)\n\n\ndef draw_clock_full(hour, minute, clock_size, bg_color, clock_color):\n    \"\"\"\n    전체 크기(clock_size x clock_size)의 시계 이미지를 그립니다.\n    반환: clock_size x clock_size x 3 크기의 numpy 배열\n    \"\"\"\n    clock_img = np.full((clock_size, clock_size, 3), clock_color, dtype=np.uint8)\n    cx, cy = clock_size // 2, clock_size // 2\n    side = clock_size\n\n    # ===== 1~12 숫자 배치 =====\n    radius_text = side * 0.4\n    for num in range(1, 13):\n        angle_deg = num * 30\n        angle_rad = math.radians(angle_deg - 90)\n        tx = int(cx + radius_text * math.cos(angle_rad))\n        ty = int(cy + radius_text * math.sin(angle_rad))\n        text = str(num)\n        (tw, th), _ = cv2.getTextSize(text, FONT, FONT_SCALE, FONT_THICKNESS)\n        tx -= tw // 2\n        ty += th // 2\n        cv2.putText(clock_img, text, (tx, ty), FONT, FONT_SCALE, (0, 0, 0),\n                     FONT_THICKNESS, cv2.LINE_AA)\n\n    # ===== 시침 그리기 =====\n    hour_angle = (hour % 12) * 30 + minute * 0.5\n    hour_length = side * 0.25\n    hour_end = get_hand_endpoint(cx, cy, hour_length, hour_angle)\n    cv2.line(clock_img, (cx, cy), hour_end, (0, 0, 0), HOUR_HAND_THICKNESS)\n\n    # ===== 분침 그리기 =====\n    minute_angle = minute * 6\n    minute_length = side * 0.425\n    minute_end = get_hand_endpoint(cx, cy, minute_length, minute_angle)\n    cv2.line(clock_img, (cx, cy), minute_end, (0, 0, 0), MINUTE_HAND_THICKNESS)\n\n    return clock_img\n\n\ndef generate_clock_image(hour, minute, apply_transform=True):\n    \"\"\"\n    시계 이미지를 생성합니다.\n    Args:\n        hour (int): 시 (1~12)\n        minute (int): 분 (0~59)\n        apply_transform (bool): 크기 조정 및 이동 적용 여부\n    Returns:\n        img (numpy array): 227x227x3 크기의 시계 이미지\n    \"\"\"\n    bg_color = generate_random_color()\n    clock_color = generate_random_color(exclude_color=bg_color)\n\n    if apply_transform:\n        # 1. 시계 크기: 이미지 크기의 0.3~0.9배\n        scale = np.random.uniform(0.3, 0.9)\n        clock_size = int(IMG_SIZE * scale)\n\n        # 2. 이동: 시계 변 길이의 0.25~0.5배 범위 내에서 랜덤 이동\n        shift_ratio = np.random.uniform(0.25, 0.5)\n        shift_amount = int(clock_size * shift_ratio)\n        direction = np.random.choice(['up', 'down', 'left', 'right'])\n\n        base_x = (IMG_SIZE - clock_size) // 2\n        base_y = (IMG_SIZE - clock_size) // 2\n\n        if direction == 'up':\n            base_y -= shift_amount\n        elif direction == 'down':\n            base_y += shift_amount\n        elif direction == 'left':\n            base_x -= shift_amount\n        elif direction == 'right':\n            base_x += shift_amount\n\n        # 3. 시계가 이미지 경계를 벗어나지 않도록 클리핑\n        base_x = max(0, min(base_x, IMG_SIZE - clock_size))\n        base_y = max(0, min(base_y, IMG_SIZE - clock_size))\n\n        clock_img = draw_clock_full(hour, minute, clock_size, bg_color, clock_color)\n        img = np.full((IMG_SIZE, IMG_SIZE, 3), bg_color, dtype=np.uint8)\n        img[base_y:base_y + clock_size, base_x:base_x + clock_size] = clock_img\n    else:\n        clock_img = draw_clock_full(hour, minute, IMG_SIZE, bg_color, clock_color)\n        img = clock_img\n\n    img = add_noise(img)\n    return img\n\n\ndef generate_dataset(num_samples, save_dir, apply_transform=True):\n    \"\"\"\n    학습/테스트용 데이터셋을 생성합니다.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    labels = []\n    for i in range(num_samples):\n        hour = np.random.randint(1, 13)\n        minute = np.random.randint(0, 60)\n        img = generate_clock_image(hour, minute, apply_transform=apply_transform)\n        filename = f\"{i:04d}_{hour:02d}_{minute:02d}.png\"\n        cv2.imwrite(os.path.join(save_dir, filename), img)\n        labels.append((hour, minute))\n    return labels\n\n\nif __name__ == \"__main__\":\n    import argparse\n    parser = argparse.ArgumentParser(description=\"시계 이미지 생성기\")\n    parser.add_argument(\"--hour\", type=int, default=3, help=\"시 (1~12)\")\n    parser.add_argument(\"--minute\", type=int, default=0, help=\"분 (0~59)\")\n    parser.add_argument(\"--num_samples\", type=int, default=1, help=\"생성할 샘플 수\")\n    parser.add_argument(\"--save_dir\", type=str, default=\"./generated_clocks\", help=\"저장 디렉토리\")\n    parser.add_argument(\"--no_transform\", action=\"store_true\", help=\"크기 조정/이동 비활성화\")\n    args = parser.parse_args()\n\n    if args.num_samples == 1:\n        img = generate_clock_image(args.hour, args.minute, apply_transform=not args.no_transform)\n        os.makedirs(args.save_dir, exist_ok=True)\n        save_path = os.path.join(args.save_dir, f\"clock_{args.hour:02d}_{args.minute:02d}.png\")\n        cv2.imwrite(save_path, img)\n        print(f\"시계 이미지 저장됨: {save_path}\")\n    else:\n        labels = generate_dataset(args.num_samples, args.save_dir,\n                                   apply_transform=not args.no_transform)\n        print(f\"{args.num_samples}개의 시계 이미지가 {args.save_dir}에 저장되었습니다.\")\n\nusage: ipykernel_launcher.py [-h] [--hour HOUR] [--minute MINUTE]\n                             [--num_samples NUM_SAMPLES] [--save_dir SAVE_DIR]\n                             [--no_transform]\nipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-c9bd2776-0221-4986-b17b-3593157a2a0e.json\n\n\n\nAn exception has occurred, use %tb to see the full traceback.\n\nSystemExit: 2\n\n\n\n\n/root/anaconda3/envs/cv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n\n\n\n\"\"\"\nProb2.py - 시계 시간 예측 딥러닝 네트워크 정의\nResNet18을 기반으로 시침/분침의 각도를 sin/cos 값으로 예측하는 회귀 네트워크입니다.\n\n접근 방식:\n- 시침 각도와 분침 각도를 각각 sin, cos 값으로 변환하여 예측 (총 4개 출력)\n- 각도를 직접 예측하면 0도와 360도 경계에서 불연속성이 발생하므로,\n  sin/cos 표현을 사용하여 이 문제를 해결합니다.\n- 사전학습된 ResNet18을 사용하여 전이학습 수행\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport math\n\n\nclass ClockNet(nn.Module):\n    \"\"\"\n    시계 시간 예측 네트워크\n    \n    입력: 227x227x3 시계 이미지\n    출력: [hour_sin, hour_cos, minute_sin, minute_cos] (4개 값)\n    \n    시침 각도: (hour % 12) * 30 + minute * 0.5  → 0~360도\n    분침 각도: minute * 6  → 0~360도\n    \"\"\"\n    \n    def __init__(self, pretrained=True):\n        super(ClockNet, self).__init__()\n        \n        # ===== 사전학습된 ResNet18 백본 =====\n        self.backbone = models.resnet18(pretrained=pretrained)\n        \n        # ResNet18의 마지막 FC 레이어 입력 차원 (512)\n        num_features = self.backbone.fc.in_features\n        \n        # 기존 FC 레이어를 새로운 회귀 헤드로 교체\n        # sin/cos 표현으로 4개 출력 (hour_sin, hour_cos, minute_sin, minute_cos)\n        self.backbone.fc = nn.Sequential(\n            nn.Linear(num_features, 256),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(256, 64),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, 4),    # [hour_sin, hour_cos, minute_sin, minute_cos]\n            nn.Tanh()            # sin/cos 값은 -1~1 범위이므로 Tanh 사용\n        )\n    \n    def forward(self, x):\n        \"\"\"\n        순전파\n        Args:\n            x: [B, 3, 227, 227] 크기의 입력 텐서\n        Returns:\n            output: [B, 4] 크기의 출력 텐서\n        \"\"\"\n        return self.backbone(x)\n\n\n# ===== 유틸리티 함수들 =====\n\ndef time_to_angles(hour, minute):\n    \"\"\"시간을 각도(도)로 변환합니다.\"\"\"\n    hour_angle = (hour % 12) * 30.0 + minute * 0.5\n    minute_angle = minute * 6.0\n    return hour_angle, minute_angle\n\n\ndef angles_to_sincos(hour_angle, minute_angle):\n    \"\"\"각도를 sin/cos 값으로 변환합니다.\"\"\"\n    h_rad = math.radians(hour_angle)\n    m_rad = math.radians(minute_angle)\n    return torch.tensor([\n        math.sin(h_rad), math.cos(h_rad),\n        math.sin(m_rad), math.cos(m_rad)\n    ], dtype=torch.float32)\n\n\ndef sincos_to_angles(output):\n    \"\"\"네트워크 출력(sin/cos)을 각도로 변환합니다.\"\"\"\n    hour_sin, hour_cos, minute_sin, minute_cos = output\n    hour_angle = math.degrees(math.atan2(hour_sin, hour_cos)) % 360\n    minute_angle = math.degrees(math.atan2(minute_sin, minute_cos)) % 360\n    return hour_angle, minute_angle\n\n\ndef angles_to_time(hour_angle, minute_angle):\n    \"\"\"각도를 시간으로 변환합니다.\"\"\"\n    minute = round(minute_angle / 6.0) % 60\n    hour = round((hour_angle - minute * 0.5) / 30.0) % 12\n    if hour == 0:\n        hour = 12\n    return hour, minute\n\n\ndef predict_time(output):\n    \"\"\"네트워크 출력에서 시간을 예측합니다.\"\"\"\n    if isinstance(output, torch.Tensor):\n        output = output.detach().cpu().numpy()\n    hour_angle, minute_angle = sincos_to_angles(output)\n    hour, minute = angles_to_time(hour_angle, minute_angle)\n    return hour, minute\n\n\nclass ClockLoss(nn.Module):\n    \"\"\"시계 시간 예측용 손실 함수 (MSE Loss)\"\"\"\n    def __init__(self):\n        super(ClockLoss, self).__init__()\n        self.mse = nn.MSELoss()\n    \n    def forward(self, pred, target):\n        return self.mse(pred, target)\n\n\nif __name__ == \"__main__\":\n    # 변환 함수 테스트\n    for h in [1, 3, 6, 9, 12]:\n        for m in [0, 15, 30, 45, 59]:\n            h_a, m_a = time_to_angles(h, m)\n            sc = angles_to_sincos(h_a, m_a)\n            ph, pm = predict_time(sc.numpy())\n            ok = \"OK\" if (ph == h and pm == m) else \"FAIL\"\n            print(f\"{ok} {h:2d}:{m:02d} -&gt; {ph:2d}:{pm:02d}\")\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[2], line 12\n      1 \"\"\"\n      2 Prob2.py - 시계 시간 예측 딥러닝 네트워크 정의\n      3 ResNet18을 기반으로 시침/분침의 각도를 sin/cos 값으로 예측하는 회귀 네트워크입니다.\n   (...)\n      9 - 사전학습된 ResNet18을 사용하여 전이학습 수행\n     10 \"\"\"\n---&gt; 12 import torch\n     13 import torch.nn as nn\n     14 import torchvision.models as models\n\nModuleNotFoundError: No module named 'torch'"
  },
  {
    "objectID": "posts/03.Review_on_PyTorch.html",
    "href": "posts/03.Review_on_PyTorch.html",
    "title": "3. Review on PyTorch",
    "section": "",
    "text": "import torch\n\nw_true = torch.Tensor([1, 2, 3])\nb_true = 5\n\nw = torch.randn(3, requires_grad = True)\nb = torch.randn(1, requires_grad = True)\n\nX = torch.randn(100, 3)\ny = torch.mv(X, w_true) + b_true\n\ngamma = 0.1\nlosses = []\n\nfor i in range(100):\n    w.grad = None\n    b.grad = None\n\n    y_pred = torch.mv(X, w) + b\n    loss = torch.mean((y- y_pred)**2)\n    loss.backward()\n    \n    w.data = w.data - gamma * w.grad.data\n    b.data = b.data - gamma * b.grad.data\n    \n    losses.append(loss.item())\n\n\nprint('obtained w: ', w, 'true w:', w_true)\nprint('obtained b: ', b, 'true b:', b_true)\n\nobtained w:  tensor([1.0000, 2.0000, 3.0000], requires_grad=True) true w: tensor([1., 2., 3.])\nobtained b:  tensor([5.0000], requires_grad=True) true b: 5\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(losses)\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\n\nw_true = torch.Tensor([1, 2, 3])\nb_true = 5\n\nnet = torch.nn.Linear(in_features = 3, out_features = 1, bias = True)\n\nX = torch.randn(100, 3)\ny = torch.mv(X, w_true) + b_true\ngamma = 0.1\nlosses = []\n\noptimizer = torch.optim.SGD(net.parameters(), lr=gamma)\nloss_fn = torch.nn.MSELoss()\n\nfor i in range(100):\n    optimizer.zero_grad()\n    \n    y_pred = net(X)\n    \n    loss = loss_fn(y_pred.squeeze(1),y)\n    loss.backward()\n    \n    optimizer.step()\n    \n    losses.append(loss.item())\n\n\nprint(list(net.parameters()))\nfrom matplotlib import pyplot as plt\nplt.plot(losses)\n\n[Parameter containing:\ntensor([[1.0000, 2.0000, 3.0000]], requires_grad=True), Parameter containing:\ntensor([5.0000], requires_grad=True)]"
  },
  {
    "objectID": "posts/03.Review_on_PyTorch.html#linear-regresssion",
    "href": "posts/03.Review_on_PyTorch.html#linear-regresssion",
    "title": "3. Review on PyTorch",
    "section": "",
    "text": "import torch\n\nw_true = torch.Tensor([1, 2, 3])\nb_true = 5\n\nw = torch.randn(3, requires_grad = True)\nb = torch.randn(1, requires_grad = True)\n\nX = torch.randn(100, 3)\ny = torch.mv(X, w_true) + b_true\n\ngamma = 0.1\nlosses = []\n\nfor i in range(100):\n    w.grad = None\n    b.grad = None\n\n    y_pred = torch.mv(X, w) + b\n    loss = torch.mean((y- y_pred)**2)\n    loss.backward()\n    \n    w.data = w.data - gamma * w.grad.data\n    b.data = b.data - gamma * b.grad.data\n    \n    losses.append(loss.item())\n\n\nprint('obtained w: ', w, 'true w:', w_true)\nprint('obtained b: ', b, 'true b:', b_true)\n\nobtained w:  tensor([1.0000, 2.0000, 3.0000], requires_grad=True) true w: tensor([1., 2., 3.])\nobtained b:  tensor([5.0000], requires_grad=True) true b: 5\n\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(losses)\n\n\n\n\n\n\n\n\n\n\n\n\nimport torch\n\nw_true = torch.Tensor([1, 2, 3])\nb_true = 5\n\nnet = torch.nn.Linear(in_features = 3, out_features = 1, bias = True)\n\nX = torch.randn(100, 3)\ny = torch.mv(X, w_true) + b_true\ngamma = 0.1\nlosses = []\n\noptimizer = torch.optim.SGD(net.parameters(), lr=gamma)\nloss_fn = torch.nn.MSELoss()\n\nfor i in range(100):\n    optimizer.zero_grad()\n    \n    y_pred = net(X)\n    \n    loss = loss_fn(y_pred.squeeze(1),y)\n    loss.backward()\n    \n    optimizer.step()\n    \n    losses.append(loss.item())\n\n\nprint(list(net.parameters()))\nfrom matplotlib import pyplot as plt\nplt.plot(losses)\n\n[Parameter containing:\ntensor([[1.0000, 2.0000, 3.0000]], requires_grad=True), Parameter containing:\ntensor([5.0000], requires_grad=True)]"
  },
  {
    "objectID": "posts/03.Review_on_PyTorch.html#multi-layer-perceptron",
    "href": "posts/03.Review_on_PyTorch.html#multi-layer-perceptron",
    "title": "3. Review on PyTorch",
    "section": "2. Multi-layer perceptron",
    "text": "2. Multi-layer perceptron\n\n\\(y = \\sigma(w_3(\\sigma(w_2(\\sigma(w_1x+b_1))+b_2))+ b_3)\\)\n\\(\\sigma\\) : Sigmoid, Tanh, ReLu and so on..\n\n\n1) Implementing Multi-layer perceptron using PyTorch\n\nimport torch\n\nnum_data = 1000\nnum_epoch = 10000\nx = torch.randn(num_data, 1)\ny = (x**2) + 3\n\nnet = torch.nn.Sequential(\ntorch.nn.Linear(1,6),\ntorch.nn.ReLU(),\ntorch.nn.Linear(6, 10),\ntorch.nn.ReLU(),\ntorch.nn.Linear(10, 6),\ntorch.nn.ReLU(),\ntorch.nn.Linear(6, 1),\n)\n\nloss_func = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01)\nlosses = []\n\nfor i in range(num_epoch):\n    optimizer.zero_grad()\n    \n    output = net(x)\n    \n    loss = loss_func(output, y)\n    loss.backward()\n    \n    optimizer.step()\n    \n    losses.append(loss.item())\n\n\nfrom matplotlib import pyplot as plt\nplt.plot(losses)\n\n\n\n\n\n\n\n\n\nx = torch.randn(5, 1)\ny = (x**2) + 3\ny_pred = net(x)\nprint(y)\nprint(y_pred)\n\ntensor([[3.1069],\n        [3.6161],\n        [3.0239],\n        [3.0206],\n        [3.0046]])\ntensor([[3.1685],\n        [3.6404],\n        [3.0821],\n        [2.9325],\n        [3.0386]], grad_fn=&lt;AddmmBackward0&gt;)"
  },
  {
    "objectID": "posts/03.Review_on_PyTorch.html#cnn",
    "href": "posts/03.Review_on_PyTorch.html#cnn",
    "title": "3. Review on PyTorch",
    "section": "3. CNN",
    "text": "3. CNN\n\n1) CNN implementation in PyTorch\n\nimport torch\nimport torch.nn\n\nclass MyCNN(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 16, 5),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(16, 32, 5),\n            torch.nn.MaxPool2d(2, 2),\n            torch.nn.Conv2d(32, 64, 5),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(2,2)\n        )\n        self.fc_layer = torch.nn.Sequential(\n            torch.nn.Linear(64*52*52, 100),\n            torch.nn.ReLU(),\n            torch.nn.Linear(100, 10)\n        )\n\n    def forward(self, x):\n        out = self.layer(x)\n        out = out.view(out.size(0), -1)\n        out = self.fc_layer(out)\n        return out\n\n\nimport numpy as np\nnum_data = 1000\nnum_epoch = 10\nx = torch.randn(num_data,1, 224,224)\ny = torch.tensor(np.random.choice([0,1], 1000), dtype = torch.long)\n\n\nnet = MyCNN()\n\nloss_func = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n\nlosses = []\n\nfor i in range(num_epoch):\n    optimizer.zero_grad()\n    \n    output = net(x)\n    \n    loss = loss_func(output, y)\n    loss.backward()\n    \n    optimizer.step()\n\n    losses.append(loss.item())\n\n\n## Cross entropy loss\nimport torch.nn as nn\n\nloss = nn.CrossEntropyLoss()\n\ninput = torch.randn(3, 5, requires_grad=True)\ntarget = torch.empty(3, dtype=torch.long).random_(5)\n\noutput = loss(input, target)\n\nprint(input)\nprint(target)\nprint(output)\n\ntensor([[-1.6207,  0.8775,  1.0726,  0.0215,  2.0341],\n        [-0.2897,  1.6222,  0.3694,  2.3245, -0.9064],\n        [-0.2044, -2.1951, -2.7583, -0.3929,  1.6202]], requires_grad=True)\ntensor([3, 3, 3])\ntensor(1.8295, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n2)Stochastic Gradient Descent\n\nGradient descent\n\nCalculate for all data\nGo 1 optimal step\n\nStochastic gradient descent\n\nCalcultate gradients for partial data\nGo many non-globally-optimal steps, but converges\n\n\n\n\n\n3) LeNet\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nimport matplotlib.pyplot as plt\n\nLEARNING_RATE = 0.001\nBATCH_SIZE = 32\nN_EPOCHS = 10\n\nIMG_SIZE = 32\nN_CLASSES = 10\n\n\nclass LeNet5(nn.Module):\n    def __init__(self, n_classes):\n        super(LeNet5, self).__init__()\n        \n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1),\n            nn.Tanh(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n            nn.Tanh(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),\n            nn.Tanh()\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(in_features=120, out_features=84),\n            nn.Tanh(),\n            nn.Linear(in_features=84, out_features=n_classes),\n        )\n\n    def forward(self, x):\n        x = self.feature_extractor(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n\ntrans = transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor()])\ntrain_dataset = datasets.MNIST(root = 'mnist_data', train=True, transform=trans, download=True)\ntest_dataset = datasets.MNIST(root = 'mnist_data', train=False, transform=trans)\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n\n\n100%|████████████████████████████████████████████████████████████████████| 9912422/9912422 [00:02&lt;00:00, 3449235.73it/s]\n\n\nExtracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n\n100%|█████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00&lt;00:00, 156533.97it/s]\n\n\nExtracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n\n100%|████████████████████████████████████████████████████████████████████| 1648877/1648877 [00:01&lt;00:00, 1194631.91it/s]\n\n\nExtracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n\n100%|██████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00&lt;00:00, 6453431.15it/s]\n\n\nExtracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\n\n\n\n\n\n\n\nA) GPU이용 x\n\ndef train(train_loader, model, criterion, optimizer):\n    model.train()\n    train_loss = 0\n    correct = 0\n    for X, y_true in train_loader:\n        optimizer.zero_grad()\n        \n        y_hat = model(X)\n        loss = criterion(y_hat, y_true)\n        \n        train_loss += loss.item()\n        pred = y_hat.argmax(dim=1, keepdim=True)\n        correct += pred.eq(y_true.view_as(pred)).sum().item()\n        \n        loss.backward()\n        optimizer.step()\n    \n    epoch_loss = train_loss / len(train_loader.dataset)\n    acc = correct / len(train_loader.dataset)\n    \n    return model, optimizer, epoch_loss, acc\n\n\ndef test(test_loader, model, criterion):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    \n    for X, y_true in test_loader:\n        \n        y_hat = model(X)\n        loss = criterion(y_hat, y_true)\n        \n        test_loss += loss.item()\n        pred = y_hat.argmax(dim=1, keepdim=True)\n        correct += pred.eq(y_true.view_as(pred)).sum().item()\n        \n    epoch_loss = test_loss / len(test_loader.dataset)\n    acc = correct / len(test_loader.dataset)\n\n    \n    return model, epoch_loss, acc\n\n\ndef training_loop(model, criterion, optimizer, train_loader, test_loader, epochs, print_every=1):\n    train_losses = []\n    test_losses = []\n    for epoch in range(epochs):\n        model, optimizer, train_loss, train_acc = train(train_loader, model, criterion, optimizer)\n        train_losses.append(train_loss)\n        with torch.no_grad():\n            model, test_loss, test_acc = test(test_loader, model, criterion)\n            test_losses.append(test_loss)\n        if epoch % print_every == (print_every - 1):\n            print(f'Epoch: {epoch}\\t'\n                f'Train loss: {train_loss:.4f}\\t'\n                f'Test loss: {test_loss:.4f}\\t'\n                f'Train accuracy: {100 * train_acc:.2f}\\t'\n                f'Test accuracy: {100 * test_acc:.2f}')\n    return model, optimizer, (train_losses, test_losses)\n\n\nmodel = LeNet5(N_CLASSES)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss()\n\nmodel, optimizer, _ = training_loop(model, criterion, optimizer, train_loader, test_loader, N_EPOCHS)\n\nEpoch: 0    Train loss: 0.0718  Test loss: 0.0717   Train accuracy: 11.12   Test accuracy: 11.44\nEpoch: 1    Train loss: 0.0712  Test loss: 0.0707   Train accuracy: 15.21   Test accuracy: 31.99\nEpoch: 2    Train loss: 0.0689  Test loss: 0.0660   Train accuracy: 49.09   Test accuracy: 62.09\nEpoch: 3    Train loss: 0.0583  Test loss: 0.0482   Train accuracy: 61.26   Test accuracy: 64.36\nEpoch: 4    Train loss: 0.0396  Test loss: 0.0317   Train accuracy: 67.64   Test accuracy: 73.92\nEpoch: 5    Train loss: 0.0275  Test loss: 0.0235   Train accuracy: 77.19   Test accuracy: 81.42\nEpoch: 6    Train loss: 0.0216  Test loss: 0.0192   Train accuracy: 82.31   Test accuracy: 84.22\nEpoch: 7    Train loss: 0.0183  Test loss: 0.0166   Train accuracy: 84.57   Test accuracy: 86.17\nEpoch: 8    Train loss: 0.0162  Test loss: 0.0149   Train accuracy: 86.02   Test accuracy: 87.38\nEpoch: 9    Train loss: 0.0147  Test loss: 0.0136   Train accuracy: 87.18   Test accuracy: 88.43\n\n\n\n\nB) GPU 이용\n\ndef train(train_loader, model, criterion, optimizer, device):\n    model.train()\n    train_loss = 0\n    correct = 0\n    for X, y_true in train_loader:\n        optimizer.zero_grad()\n\n        X = X.to(device)\n        y_true = y_true.to(device)\n\n        y_hat = model(X)\n        loss = criterion(y_hat, y_true)\n        \n        train_loss += loss.item()\n        pred = y_hat.argmax(dim=1, keepdim=True)\n        correct += pred.eq(y_true.view_as(pred)).sum().item()\n        \n        loss.backward()\n        optimizer.step()\n    \n    epoch_loss = train_loss / len(train_loader.dataset)\n    acc = correct / len(train_loader.dataset)\n    \n    return model, optimizer, epoch_loss, acc\n\n\ndef test(test_loader, model, criterion, device):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    \n    for X, y_true in test_loader:\n        \n        X = X.to(device)\n        y_true = y_true.to(device)\n        \n        y_hat = model(X)\n        loss = criterion(y_hat, y_true)\n        \n        test_loss += loss.item()\n        pred = y_hat.argmax(dim=1, keepdim=True)\n        correct += pred.eq(y_true.view_as(pred)).sum().item()\n        \n    epoch_loss = test_loss / len(test_loader.dataset)\n    acc = correct / len(test_loader.dataset)\n\n    \n    return model, epoch_loss, acc\n\n\ndef training_loop(model, criterion, optimizer, train_loader, test_loader, epochs, device, print_every=1):\n    train_losses = []\n    test_losses = []\n    for epoch in range(epochs):\n        model, optimizer, train_loss, train_acc = train(train_loader, model, criterion, optimizer, device)\n        train_losses.append(train_loss)\n        with torch.no_grad():\n            model, test_loss, test_acc = test(test_loader, model, criterion, device)\n            test_losses.append(test_loss)\n        if epoch % print_every == (print_every - 1):\n            print(f'Epoch: {epoch}\\t'\n                f'Train loss: {train_loss:.4f}\\t'\n                f'Test loss: {test_loss:.4f}\\t'\n                f'Train accuracy: {100 * train_acc:.2f}\\t'\n                f'Test accuracy: {100 * test_acc:.2f}')\n    return model, optimizer, (train_losses, test_losses)\n\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nDEVICE\n\n'cuda'\n\n\n\nmodel = LeNet5(N_CLASSES).to(DEVICE)\n\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss()\n\nmodel, optimizer, _ = training_loop(model, criterion, optimizer, train_loader, test_loader, N_EPOCHS, DEVICE)\n\nEpoch: 0    Train loss: 0.0717  Test loss: 0.0715   Train accuracy: 12.12   Test accuracy: 24.27\nEpoch: 1    Train loss: 0.0707  Test loss: 0.0696   Train accuracy: 35.98   Test accuracy: 40.96\nEpoch: 2    Train loss: 0.0659  Test loss: 0.0596   Train accuracy: 46.88   Test accuracy: 53.71\nEpoch: 3    Train loss: 0.0504  Test loss: 0.0404   Train accuracy: 60.58   Test accuracy: 69.89\nEpoch: 4    Train loss: 0.0336  Test loss: 0.0276   Train accuracy: 74.09   Test accuracy: 79.12\nEpoch: 5    Train loss: 0.0245  Test loss: 0.0213   Train accuracy: 81.22   Test accuracy: 83.96\nEpoch: 6    Train loss: 0.0199  Test loss: 0.0178   Train accuracy: 84.45   Test accuracy: 86.18\nEpoch: 7    Train loss: 0.0171  Test loss: 0.0156   Train accuracy: 86.00   Test accuracy: 87.37\nEpoch: 8    Train loss: 0.0153  Test loss: 0.0141   Train accuracy: 87.15   Test accuracy: 88.03\nEpoch: 9    Train loss: 0.0140  Test loss: 0.0130   Train accuracy: 87.95   Test accuracy: 88.87\n\n\n\n\n\n4) ResNet\n\nUse network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping\n\n\n\ndef conv_block_1(in_dim,out_dim,act_fn):\n    model = nn.Sequential(\n        nn.Conv2d(in_dim,out_dim, kernel_size=1, stride=1),\n        act_fn,\n    )\n    return model\n    \ndef conv_block_1_stride_2(in_dim,out_dim,act_fn):\n    model = nn.Sequential(\n        nn.Conv2d(in_dim,out_dim, kernel_size=1, stride=2),\n        act_fn,\n    )\n    return model\n    \ndef conv_block_1_n(in_dim,out_dim):\n    model = nn.Sequential(\n        nn.Conv2d(in_dim,out_dim, kernel_size=1, stride=1),\n    )\n    return model\n\ndef conv_block_1_stride_2_n(in_dim,out_dim):\n    model = nn.Sequential(\n        nn.Conv2d(in_dim,out_dim, kernel_size=1, stride=2),\n    )\n    return model\n\ndef conv_block_3(in_dim,out_dim,act_fn):\n    model = nn.Sequential(\n        nn.Conv2d(in_dim,out_dim, kernel_size=3, stride=1, padding=1),\n        act_fn,\n    )\n    return model\n\n\nclass BottleNeck(nn.Module):\n    def __init__(self,in_dim,mid_dim,out_dim,act_fn):\n        super(BottleNeck,self).__init__()\n        self.layer = nn.Sequential(\n            conv_block_1(in_dim,mid_dim,act_fn),\n            conv_block_3(mid_dim,mid_dim,act_fn),\n            conv_block_1_n(mid_dim,out_dim),\n        )\n        self.downsample = nn.Conv2d(in_dim,out_dim,1,1)\n        \n    def forward(self,x):\n        downsample = self.downsample(x)\n        out = self.layer(x)\n        out = out + downsample\n        \n        return out\n    \nclass BottleNeck_no_down(nn.Module):\n    def __init__(self,in_dim,mid_dim,out_dim,act_fn):\n        super(BottleNeck_no_down,self).__init__()\n        self.layer = nn.Sequential(\n            conv_block_1(in_dim,mid_dim,act_fn),\n            conv_block_3(mid_dim,mid_dim,act_fn),\n            conv_block_1_n(mid_dim,out_dim),\n        )\n    def forward(self,x):\n        out = self.layer(x)\n        out = out + x\n        \n        return out\n\n\nclass BottleNeck_stride(nn.Module):\n    def __init__(self,in_dim,mid_dim,out_dim,act_fn):\n        \n        super(BottleNeck_stride,self).__init__()\n        self.layer = nn.Sequential(\n            conv_block_1_stride_2(in_dim,mid_dim,act_fn),\n            conv_block_3(mid_dim,mid_dim,act_fn),\n            conv_block_1_n(mid_dim,out_dim),\n        )\n        self.downsample = nn.Conv2d(in_dim,out_dim,1,2)\n    def forward(self,x):\n        \n        downsample = self.downsample(x)\n        out = self.layer(x)\n        out = out + downsample\n        \n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, base_dim, num_classes=2):\n        super(ResNet, self).__init__()\n        self.act_fn = nn.ReLU()\n        \n        self.layer_1 = nn.Sequential(\n            nn.Conv2d(3,base_dim,7,2,3),\n            nn.ReLU(),\n            nn.MaxPool2d(3,2,1),\n        )\n        \n        self.layer_2 = nn.Sequential(\n            BottleNeck(base_dim,base_dim,base_dim*4,self.act_fn),\n            BottleNeck_no_down(base_dim*4,base_dim,base_dim*4,self.act_fn),\n            BottleNeck_stride(base_dim*4,base_dim,base_dim*4,self.act_fn),\n        )\n        self.layer_3 = nn.Sequential(\n            BottleNeck(base_dim*4,base_dim*2,base_dim*8,self.act_fn),\n            BottleNeck_no_down(base_dim*8,base_dim*2,base_dim*8,self.act_fn),\n            BottleNeck_no_down(base_dim*8,base_dim*2,base_dim*8,self.act_fn),\n            BottleNeck_stride(base_dim*8,base_dim*2,base_dim*8,self.act_fn),\n        )\n        self.layer_4 = nn.Sequential(\n            BottleNeck(base_dim*8,base_dim*4,base_dim*16,self.act_fn),\n            BottleNeck_no_down(base_dim*16,base_dim*4,base_dim*16,self.act_fn),\n            BottleNeck_no_down(base_dim*16,base_dim*4,base_dim*16,self.act_fn),\n            BottleNeck_no_down(base_dim*16,base_dim*4,base_dim*16,self.act_fn),\n            BottleNeck_no_down(base_dim*16,base_dim*4,base_dim*16,self.act_fn),\n            BottleNeck_stride(base_dim*16,base_dim*4,base_dim*16,self.act_fn),\n        )\n        self.layer_5 = nn.Sequential(\n            BottleNeck(base_dim*16,base_dim*8,base_dim*32,nn.ReLU()),\n            BottleNeck_no_down(base_dim*32,base_dim*8,base_dim*32,self.act_fn),\n            BottleNeck(base_dim*32,base_dim*8,base_dim*32,self.act_fn),\n        )\n        self.avgpool = nn.AvgPool2d(7,1)\n        self.fc_layer = nn.Linear(base_dim*32,num_classes)\n    \n    def forward(self, x):\n        out = self.layer_1(x)\n        out = self.layer_2(out)\n        out = self.layer_3(out)\n        out = self.layer_4(out)\n        out = self.layer_5(out)\n        out = self.avgpool(out)\n        out = out.view(batch_size,-1)\n        out = self.fc_layer(out)\n        \n        return out"
  }
]