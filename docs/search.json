[
  {
    "objectID": "posts/01.Overview_and_OpenCV.html",
    "href": "posts/01.Overview_and_OpenCV.html",
    "title": "01. Overview and Open CV",
    "section": "",
    "text": "Understand what is computer vision/deep learning algorithms\nUnderstand how to implement image/video processing algorithms in Python using OpenCV.\nUnderstand how to implement/train/test convolutional neural networks (CNNs) and Transformer in PyTorch.\n\n\n\n\n- What is computer vision?\n\nImage Processing, Optics, Machine Learning, Graphics, Algorihm, System Architecture, Robotics, Speech NLP\n\n- What is deep learning?\n\nAlphaGo is using a deep learning when training their parameters\nImageNet challenge (2012년 AlexNet(8 layers) \\(\\rightarrow\\) 2015년 ResNet(152 layers)) getting deeper\n\n\n\n\n\n- Image classification for Cifar-10 dataset.\n\nAssuming that images are composed of one main object.\n\n\n- Object detection\n\nImage could be composed of multiple objects\nDetect object location + class imformation\n\n\n- Semantic segmentation\n\nDetect object’s pixel-level location + class information\n\n\n- Image generation\n\ndraw new images by changing atrributes of the original images\n\n\n- Pose estimation\n\nRecognize 2D/3D poses of the main objects(body, hand, face)\n\n\n- 3D hand mesh reconstruction\n\n21 Keypoints(4points for finger(20 joint) + wrist(손목)(1 joint))\n778 vertices\n\n\n- 3D human mesh reconstruction\n\nabout 7000 vertices"
  },
  {
    "objectID": "posts/01.Overview_and_OpenCV.html#course-overview",
    "href": "posts/01.Overview_and_OpenCV.html#course-overview",
    "title": "01. Overview and Open CV",
    "section": "",
    "text": "Understand what is computer vision/deep learning algorithms\nUnderstand how to implement image/video processing algorithms in Python using OpenCV.\nUnderstand how to implement/train/test convolutional neural networks (CNNs) and Transformer in PyTorch.\n\n\n\n\n- What is computer vision?\n\nImage Processing, Optics, Machine Learning, Graphics, Algorihm, System Architecture, Robotics, Speech NLP\n\n- What is deep learning?\n\nAlphaGo is using a deep learning when training their parameters\nImageNet challenge (2012년 AlexNet(8 layers) \\(\\rightarrow\\) 2015년 ResNet(152 layers)) getting deeper\n\n\n\n\n\n- Image classification for Cifar-10 dataset.\n\nAssuming that images are composed of one main object.\n\n\n- Object detection\n\nImage could be composed of multiple objects\nDetect object location + class imformation\n\n\n- Semantic segmentation\n\nDetect object’s pixel-level location + class information\n\n\n- Image generation\n\ndraw new images by changing atrributes of the original images\n\n\n- Pose estimation\n\nRecognize 2D/3D poses of the main objects(body, hand, face)\n\n\n- 3D hand mesh reconstruction\n\n21 Keypoints(4points for finger(20 joint) + wrist(손목)(1 joint))\n778 vertices\n\n\n- 3D human mesh reconstruction\n\nabout 7000 vertices"
  },
  {
    "objectID": "posts/01.Overview_and_OpenCV.html#open-cv",
    "href": "posts/01.Overview_and_OpenCV.html#open-cv",
    "title": "01. Overview and Open CV",
    "section": "2. Open CV",
    "text": "2. Open CV\n\nA. Image read\n- Import\n\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\n- 컬러 이미지로 불러오기 IMREAD_UNCHANGED\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n\nimg.shape\n\n(454, 680, 3)\n\n\n- 흑백 이미지로 불러오기 IMREAD_GRAYSCALE\n\nimg2 = cv2.imread('./example.jpg', cv2.IMREAD_GRAYSCALE)\n\nplt.imshow(img2, cmap = 'gray')\n\n\n\n\n\n\n\n\n\nimg2.shape\n\n(454, 680)\n\n\n\n\nB.Image processing\n- 사각형 추가하기\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\ncv2.rectangle(img, (50, 50), (150, 150), (255, 0, 0))\ncv2.rectangle(img, (300, 300), (100, 100), (0, 255, 0), 10)\ncv2.rectangle(img, (450, 200), (200, 450), (0, 0, 255), -1)\n\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- polylines 그리기\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\npts1 = np.array([[50,50], [150,150], [100,140], [200,240]], dtype=np.int32)\ncv2.polylines(img, [pts1], False, (255,0,0))\n\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- 원 그리기\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\ncv2.circle(img, (150,150), 100, (255,0,0))\ncv2.ellipse(img, ((325,300), (150,100), 0), (0,255,0), 5)\n\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- Put texts\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\ncv2.putText(img, 'UNIST CSE48001 Computer Vision!', (20,50), cv2.FONT_HERSHEY_PLAIN, 2, (0,255,0))\n\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- Image resize\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\nheight, width = img.shape[:2]\n\ndst1 = cv2.resize(img, None, None, 0.5, 0.5, cv2.INTER_CUBIC)\ndst2 = cv2.resize(img, None, None, 2, 2, cv2.INTER_CUBIC)\n\nimg_rgb_dst1 = cv2.cvtColor(dst1, cv2.COLOR_BGR2RGB)\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nimg_rgb_dst2 = cv2.cvtColor(dst2, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(5, 5))\nplt.imshow(img_rgb_dst1)\nplt.title('0.5x scaled')\nplt.axis('off')\nplt.show()\n\nplt.figure(figsize=(10, 10))\nplt.imshow(img_rgb)\nplt.title('Original')\nplt.axis('off')\nplt.show()\n\nplt.figure(figsize=(20, 20))\nplt.imshow(img_rgb_dst2)\nplt.title('2x scaled')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n- Image blurring\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\nblur1 = cv2.blur(img, (10,10))\n\nimg_rgb = cv2.cvtColor(blur1, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- Edge detection(밝기가 급격히 변하는 부분 감지)\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\nedges = cv2.Canny(img, 100, 200)\n\nimg_rgb = cv2.cvtColor(edges, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- Corner detection\n\nimg = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\n\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\ncorner = cv2.cornerHarris(gray, 2, 3, 0.04)\ncoord = np.where(corner &gt; 0.1*corner.max())\ncoord = np.stack((coord[1], coord[0]), axis=-1)\n\nfor x,y in coord:\n  cv2.circle(img, (x,y), 5, (0,0,255), 1, cv2.LINE_AA)\n\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img_rgb)\n\n\n\n\n\n\n\n\n- Image matching\n\nimg1 = cv2.imread('./example.jpg', cv2.IMREAD_UNCHANGED)\nimg2 = cv2.imread('./example4.jpg', cv2.IMREAD_UNCHANGED)\n\ngray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\ngray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n\ndetector = cv2.ORB_create()\nkp1, desc1 = detector.detectAndCompute(gray1, None)\nkp2, desc2 = detector.detectAndCompute(gray2, None)\n\nmatcher = cv2.BFMatcher(cv2.NORM_L1, crossCheck = True)\nmatches = matcher.match(desc1, desc2)\n\nres = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n\nimg_rgb = cv2.cvtColor(res, cv2.COLOR_BGR2RGB)\nplt.imshow(img_rgb)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CV",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 20, 2026\n\n\n01. Overview and Open CV\n\n\n이상민 \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]